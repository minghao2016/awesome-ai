{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:03:14.786147Z",
     "start_time": "2019-03-25T11:03:09.952910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_boston\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:03:23.695103Z",
     "start_time": "2019-03-25T11:03:23.674556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "\n",
    "features = np.array(boston.data)\n",
    "labels = np.array(boston.target)\n",
    "\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:03:29.411942Z",
     "start_time": "2019-03-25T11:03:29.405848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels.reshape((len(labels), 1)) # reshape rank 1 array to have 1 dim\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:03:31.454529Z",
     "start_time": "2019-03-25T11:03:31.448971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "       4.980e+00])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:03:31.743903Z",
     "start_time": "2019-03-25T11:03:31.735084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:03:53.730739Z",
     "start_time": "2019-03-25T11:03:53.723155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.mean(features, axis=0) # Feature wise mean\n",
    "std = np.std(features, axis=0) # Feature wise standard deviation\n",
    "\n",
    "# Normalize now\n",
    "features = (features - mean) / std\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:03:54.986924Z",
     "start_time": "2019-03-25T11:03:54.979033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.41978194,  0.28482986, -1.2879095 , -0.27259857, -0.14421743,\n",
       "        0.41367189, -0.12001342,  0.1402136 , -0.98284286, -0.66660821,\n",
       "       -1.45900038,  0.44105193, -1.0755623 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:04:25.144475Z",
     "start_time": "2019-03-25T11:04:25.137105Z"
    }
   },
   "outputs": [],
   "source": [
    "random_indices = np.random.rand(len(features)) < 0.80 # Random indexes for splitting dataset\n",
    "\n",
    "# New train set\n",
    "train_x = features[random_indices]\n",
    "train_y = labels[random_indices]\n",
    "\n",
    "# New validation set\n",
    "val_x = features[~random_indices]\n",
    "val_y = labels[~random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:04:40.090910Z",
     "start_time": "2019-03-25T11:04:40.083642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((402, 13), (402, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:04:47.263069Z",
     "start_time": "2019-03-25T11:04:47.257812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((104, 13), (104, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:05:26.430573Z",
     "start_time": "2019-03-25T11:05:26.424878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = features.shape[1] # number of features\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:05:39.902151Z",
     "start_time": "2019-03-25T11:05:39.849601Z"
    }
   },
   "outputs": [],
   "source": [
    "# create placeholders for inputs\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, num_features), name=\"feature_matrix\") # num_samples x num_features\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1), name=\"target_vector\") # num_sampels x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:06:03.524993Z",
     "start_time": "2019-03-25T11:06:03.513146Z"
    }
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.zeros(dtype=tf.float32, shape=(1, num_features))) # Initialize weights with all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:06:21.232111Z",
     "start_time": "2019-03-25T11:06:21.217651Z"
    }
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.zeros(dtype=tf.float32, shape=(1, num_features))) # Set bias to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:06:35.860517Z",
     "start_time": "2019-03-25T11:06:35.852353Z"
    }
   },
   "outputs": [],
   "source": [
    "# linear regression model\n",
    "linear_model = tf.add(tf.multiply(w, x), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:07:16.231135Z",
     "start_time": "2019-03-25T11:07:16.221500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set loss function\n",
    "loss_function = tf.reduce_mean(tf.square(linear_model - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using inbuilt optimizer for minimizing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:07:44.560457Z",
     "start_time": "2019-03-25T11:07:44.407423Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:11:38.678934Z",
     "start_time": "2019-03-25T11:11:16.845136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "At step 0, train_loss is 575.0603637695312\n",
      "At step 1000, train_loss is 238.51412963867188\n",
      "At step 2000, train_loss is 104.8044662475586\n",
      "At step 3000, train_loss is 66.54669952392578\n",
      "At step 4000, train_loss is 61.44092559814453\n",
      "At step 5000, train_loss is 61.2686882019043\n",
      "At step 6000, train_loss is 61.26815414428711\n",
      "At step 7000, train_loss is 61.26813507080078\n",
      "At step 8000, train_loss is 61.268150329589844\n",
      "At step 9000, train_loss is 61.26815414428711\n",
      "At step 10000, train_loss is 61.26817321777344\n",
      "Finally, val_loss is 89.43696594238281\n"
     ]
    }
   ],
   "source": [
    "# Create new session\n",
    "with tf.Session() as sess:\n",
    "    epochs = 10000 # number of iterations over the whole train set\n",
    "    sess.run(tf.global_variables_initializer()) # Initialize global variables\n",
    "    print(\"Training...\")\n",
    "\n",
    "    # Run the optimizer\n",
    "    for i in range(epochs+1):\n",
    "        # Update weights and bais\n",
    "        sess.run(trainer, feed_dict={x:train_x, y:train_y})\n",
    "        # print loss at each 1000th epoch\n",
    "        if i % 1000 == 0:\n",
    "            # Get training loss value at each 1000th epoch\n",
    "            c = sess.run(loss_function, feed_dict={x:train_x, y:train_y})\n",
    "            print(\"At step {}, train_loss is {}\".format(i, c))\n",
    "    # Get loss on the validation set\n",
    "    print(\"Finally, val_loss is {}\".format(sess.run(loss_function, feed_dict={x:val_x, y:val_y})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:12:23.087068Z",
     "start_time": "2019-03-25T11:12:23.000740Z"
    }
   },
   "outputs": [],
   "source": [
    "grad_w, grad_b = tf.gradients(xs=[w, b], ys=loss_function) # Get gradients\n",
    "\n",
    "# Adjust weights and bias\n",
    "new_w = w.assign(w - 0.01 * grad_w)\n",
    "new_b = b.assign(b - 0.01 * grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T11:14:50.628159Z",
     "start_time": "2019-03-25T11:14:25.530149Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "At step 0, train_loss is 575.5902709960938\n",
      "At step 1000, train_loss is 84.65240478515625\n",
      "At step 2000, train_loss is 62.35044860839844\n",
      "At step 3000, train_loss is 61.319297790527344\n",
      "At step 4000, train_loss is 61.27061080932617\n",
      "At step 5000, train_loss is 61.26823806762695\n",
      "At step 6000, train_loss is 61.268123626708984\n",
      "At step 7000, train_loss is 61.26814270019531\n",
      "At step 8000, train_loss is 61.26813507080078\n",
      "At step 9000, train_loss is 61.26814270019531\n",
      "At step 10000, train_loss is 61.26814270019531\n",
      "Finally, val_loss is 89.43856811523438\n"
     ]
    }
   ],
   "source": [
    "# Create new session\n",
    "with tf.Session() as sess:\n",
    "    epochs = 10000\n",
    "    # Initialize global variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    # Train\n",
    "    for i in range(epochs+1):\n",
    "        _, _, c = sess.run([new_w, new_b, loss_function], feed_dict={x:train_x, y:train_y})\n",
    "        # At each 1000th epoch prompt training loss\n",
    "        if i % 1000 == 0:\n",
    "            print(\"At step {}, train_loss is {}\".format(i, c))\n",
    "    # Get loss on the validation set\n",
    "    print(\"Finally, val_loss is {}\".format(sess.run(loss_function, feed_dict={x:val_x, y:val_y})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai_env]",
   "language": "python",
   "name": "conda-env-ai_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

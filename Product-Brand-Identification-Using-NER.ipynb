{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T05:06:57.078231Z",
     "start_time": "2019-02-24T05:06:57.074450Z"
    }
   },
   "source": [
    "Named Entity Recognition model tries to identify \"Brand\" names from e-comm product titles. The dataset is taken from flipkart website. It consists of about 4Lakh+ de-duplicated samples spanning multiple product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T15:26:05.635726Z",
     "start_time": "2019-02-26T15:26:04.175381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load packages\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import string\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T15:26:12.828469Z",
     "start_time": "2019-02-26T15:26:05.638832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772629, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "FILEPATH = \"/Users/nityansuman/__data__/flipkart_dataset/product_identification_train_set.csv\"\n",
    "data = pd.read_csv(FILEPATH)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:28:28.855792Z",
     "start_time": "2019-02-23T20:28:28.841616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_path</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_no</th>\n",
       "      <th>category</th>\n",
       "      <th>present_in_title</th>\n",
       "      <th>present_in_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Home Improvement &gt;&gt; Hardware &gt;&gt; Bathroom &amp; Ki...</td>\n",
       "      <td>Hindware Vara Spa Shower Head</td>\n",
       "      <td>Buy Hindware Vara Spa Shower Head for Rs.1567 ...</td>\n",
       "      <td>Hindware</td>\n",
       "      <td>Vara Spa</td>\n",
       "      <td>F160050</td>\n",
       "      <td>hardware</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Home Improvement &gt;&gt; Hardware &gt;&gt; Bathroom &amp; Ki...</td>\n",
       "      <td>Sunrise 8 inch Ultra Thin With 15 inch Brass A...</td>\n",
       "      <td>Buy Sunrise 8 inch Ultra Thin With 15 inch Bra...</td>\n",
       "      <td>Sunrise</td>\n",
       "      <td>8 inch Ultra Thin With 15 inch Brass Arm</td>\n",
       "      <td>8inslimwith15inbras</td>\n",
       "      <td>hardware</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Home Improvement &gt;&gt; Hardware &gt;&gt; Bathroom &amp; Ki...</td>\n",
       "      <td>Polytuf Overhead- 4 Inches Chrome Plated Showe...</td>\n",
       "      <td>Buy Polytuf Overhead- 4 Inches Chrome Plated S...</td>\n",
       "      <td>Polytuf</td>\n",
       "      <td>Overhead- 4 Inches Chrome Plated</td>\n",
       "      <td>1067(a)</td>\n",
       "      <td>hardware</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       category_path  \\\n",
       "0  [Home Improvement >> Hardware >> Bathroom & Ki...   \n",
       "1  [Home Improvement >> Hardware >> Bathroom & Ki...   \n",
       "2  [Home Improvement >> Hardware >> Bathroom & Ki...   \n",
       "\n",
       "                                               title  \\\n",
       "0                      Hindware Vara Spa Shower Head   \n",
       "1  Sunrise 8 inch Ultra Thin With 15 inch Brass A...   \n",
       "2  Polytuf Overhead- 4 Inches Chrome Plated Showe...   \n",
       "\n",
       "                                         description     brand  \\\n",
       "0  Buy Hindware Vara Spa Shower Head for Rs.1567 ...  Hindware   \n",
       "1  Buy Sunrise 8 inch Ultra Thin With 15 inch Bra...   Sunrise   \n",
       "2  Buy Polytuf Overhead- 4 Inches Chrome Plated S...   Polytuf   \n",
       "\n",
       "                                      model             model_no  category  \\\n",
       "0                                  Vara Spa              F160050  hardware   \n",
       "1  8 inch Ultra Thin With 15 inch Brass Arm  8inslimwith15inbras  hardware   \n",
       "2          Overhead- 4 Inches Chrome Plated              1067(a)  hardware   \n",
       "\n",
       "   present_in_title  present_in_description  \n",
       "0              True                    True  \n",
       "1              True                    True  \n",
       "2              True                    True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:28:33.124221Z",
     "start_time": "2019-02-23T20:28:32.766743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404024, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset=[\"title\", \"brand\"], keep=False)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:28:50.284704Z",
     "start_time": "2019-02-23T20:28:50.066436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_path                0\n",
       "title                        0\n",
       "description                  0\n",
       "brand                        3\n",
       "model                        0\n",
       "model_no                    76\n",
       "category                  4626\n",
       "present_in_title             0\n",
       "present_in_description       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null or missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:29:01.714362Z",
     "start_time": "2019-02-23T20:29:01.600029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404021, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with null values based on brands\n",
    "data = data.dropna(axis=0, subset=[\"brand\"], how=\"any\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:29:25.774631Z",
     "start_time": "2019-02-23T20:29:23.288278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_path</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_no</th>\n",
       "      <th>category</th>\n",
       "      <th>present_in_title</th>\n",
       "      <th>present_in_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404021</td>\n",
       "      <td>404021</td>\n",
       "      <td>404021</td>\n",
       "      <td>404021</td>\n",
       "      <td>404021</td>\n",
       "      <td>403945</td>\n",
       "      <td>399395</td>\n",
       "      <td>404021</td>\n",
       "      <td>404021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>127114</td>\n",
       "      <td>403879</td>\n",
       "      <td>403880</td>\n",
       "      <td>22096</td>\n",
       "      <td>305259</td>\n",
       "      <td>245074</td>\n",
       "      <td>356</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[Jewellery &gt;&gt; Earrings]</td>\n",
       "      <td>Ciba Vision Freshlook Color Blends Monthly Con...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Bike Handle Grip</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>earrings</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>35585</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "      <td>10311</td>\n",
       "      <td>2068</td>\n",
       "      <td>129437</td>\n",
       "      <td>38030</td>\n",
       "      <td>384445</td>\n",
       "      <td>403645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  category_path  \\\n",
       "count                    404021   \n",
       "unique                   127114   \n",
       "top     [Jewellery >> Earrings]   \n",
       "freq                      35585   \n",
       "\n",
       "                                                    title description  \\\n",
       "count                                              404021      404021   \n",
       "unique                                             403879      403880   \n",
       "top     Ciba Vision Freshlook Color Blends Monthly Con...   Not Found   \n",
       "freq                                                    5          63   \n",
       "\n",
       "            brand             model   model_no  category present_in_title  \\\n",
       "count      404021            404021     403945    399395           404021   \n",
       "unique      22096            305259     245074       356                2   \n",
       "top     Not Found  Bike Handle Grip  Not Found  earrings             True   \n",
       "freq        10311              2068     129437     38030           384445   \n",
       "\n",
       "       present_in_description  \n",
       "count                  404021  \n",
       "unique                      2  \n",
       "top                      True  \n",
       "freq                   403645  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:31:31.477399Z",
     "start_time": "2019-02-23T20:30:57.867689Z"
    }
   },
   "outputs": [],
   "source": [
    "word_frequency_mapping = dict()\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    # iterate over all samples\n",
    "    raw_string = row[\"title\"]\n",
    "    \n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", raw_string) # access only characters\n",
    "    tokens = text.split() # tokenize string into words\n",
    "    \n",
    "    for tok in tokens:\n",
    "        # access each token\n",
    "        if tok not in word_frequency_mapping:\n",
    "            # if not in vocab, add new word\n",
    "            word_frequency_mapping[tok] = 1\n",
    "        else:\n",
    "            # if word present in vocab, update frequency\n",
    "            word_frequency_mapping[tok] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:31:42.902595Z",
     "start_time": "2019-02-23T20:31:42.740285Z"
    }
   },
   "outputs": [],
   "source": [
    "# reverse the dict based on the values\n",
    "sorted_word_frequency_mapping = sorted(word_frequency_mapping.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:31:43.691018Z",
     "start_time": "2019-02-23T20:31:43.687402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Words =  95416 95416\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Unique Words = \", len(word_frequency_mapping), len(sorted_word_frequency_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:32:14.853224Z",
     "start_time": "2019-02-23T20:32:14.808590Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a vocabulary for word embedding\n",
    "\n",
    "# add word for unknown words and for padding\n",
    "word2idx = {\"<PAD>\":0, \"<UNK>\": 1}\n",
    "for i in range(len(sorted_word_frequency_mapping)):\n",
    "    word2idx[sorted_word_frequency_mapping[i][0]] = i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:33:18.793034Z",
     "start_time": "2019-02-23T20:33:18.776277Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a mapping from index to word\n",
    "idx2word = {val:key for key, val in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:33:19.488472Z",
     "start_time": "2019-02-23T20:33:19.484844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens in Word Vocab =  95418 95418\n"
     ]
    }
   ],
   "source": [
    "# check for the lenght of the vocab\n",
    "print(\"Number of Unique Tokens in Word Vocab = \", len(idx2word), len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:34:52.541927Z",
     "start_time": "2019-02-23T20:34:52.537823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique character set for character embedding\n",
    "chars = string.ascii_letters\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:35:09.440979Z",
     "start_time": "2019-02-23T20:35:09.437599Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a character vocabulary\n",
    "char2idx = {\"<UNK>\": 1, \"<PAD>\": 0}\n",
    "for i in range(len(chars)):\n",
    "    char2idx[chars[i]] = i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:35:13.745060Z",
     "start_time": "2019-02-23T20:35:13.742072Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a mapping from index to word\n",
    "idx2char = {val:key for key, val in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:35:42.452076Z",
     "start_time": "2019-02-23T20:35:42.448301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens in Character Vocab =  54 54\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Unique Tokens in Character Vocab = \", len(char2idx), len(idx2char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Character Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:39:20.512366Z",
     "start_time": "2019-02-23T20:38:08.111922Z"
    }
   },
   "outputs": [],
   "source": [
    "x_char = list()\n",
    "max_len = 16 # number of max words\n",
    "max_char_len = 8 # number of max characters in each word\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    # access each sample\n",
    "    text = row[\"title\"]\n",
    "    \n",
    "    text = re.sub(\"[^a-zA-Z0-9 ]\", \" \", text) # remove all non english characters\n",
    "    tokens = text.split() # tokenize string\n",
    "    \n",
    "    total_token = list()\n",
    "    # create character level code for each word\n",
    "    for k in range(max_len):\n",
    "        word_seq = list()\n",
    "        for j in range(max_char_len):\n",
    "            try:\n",
    "                word_seq.append(char2idx[tokens[k][j]])\n",
    "            except:\n",
    "                word_seq.append(char2idx[\"<PAD>\"])\n",
    "        total_token.append(word_seq)\n",
    "    x_char.append(total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:40:11.801559Z",
     "start_time": "2019-02-23T20:40:11.796320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404021, 16, 8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_char), len(x_char[0]), len(x_char[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:40:52.592405Z",
     "start_time": "2019-02-23T20:40:18.239241Z"
    }
   },
   "outputs": [],
   "source": [
    "x_word = list()\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    # access each sample\n",
    "    text = row[\"title\"]\n",
    "    \n",
    "    text = re.sub(\"[^a-zA-Z0-9 ]\", \" \", text) # remove all non english characters\n",
    "    tokens = text.split() # tokenize string\n",
    "    \n",
    "    total_tokens=  list()\n",
    "    # create word level code\n",
    "    for tok in tokens:\n",
    "        if tok in word2idx.keys():\n",
    "            total_tokens.append(word2idx[tok])\n",
    "        else:\n",
    "            total_tokens.append(word2idx[\"<UNK>\"])\n",
    "    x_word.append(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:40:52.599758Z",
     "start_time": "2019-02-23T20:40:52.594950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404021, 12)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_word), len(x_word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Brand Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:41:50.624804Z",
     "start_time": "2019-02-23T20:41:50.621705Z"
    }
   },
   "outputs": [],
   "source": [
    "# mapping for brand names\n",
    "tag2idx = {\n",
    "    \"O\": 0, # others\n",
    "    \"B-M\": 1, # begin of brand name \n",
    "    \"I-M\": 2 # intermediate brand name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:46:04.123791Z",
     "start_time": "2019-02-23T20:45:24.258567Z"
    }
   },
   "outputs": [],
   "source": [
    "y = list()\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    # access each sample\n",
    "    name, title = row[\"brand\"], row[\"title\"] # access brand name and product title\n",
    "    \n",
    "    name = re.sub(\"[^a-zA-Z0-9 ]\", \" \", str(name)) # remove all non english chars from brand name\n",
    "    name_tokens = name.split() # tokenize brand name\n",
    "\n",
    "    title = re.sub(\"[^a-zA-Z0-9 ]\", \" \", str(title)) # remove all non english chars from title\n",
    "    title_tokens = title.split() # tokenize title string\n",
    "\n",
    "    tags = list()\n",
    "    for t in title_tokens:\n",
    "        # access each toke\n",
    "        if t not in name_tokens:\n",
    "            # tag other than brand name\n",
    "            tags.append(tag2idx[\"O\"])\n",
    "        elif name_tokens.index(t) == 0:\n",
    "            # tag begining brand name\n",
    "            tags.append(tag2idx[\"B-M\"])\n",
    "        else:\n",
    "            # tag intermediate brand name\n",
    "            tags.append(tag2idx[\"I-M\"])\n",
    "    y.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:47:02.199688Z",
     "start_time": "2019-02-23T20:47:02.195337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404021, 12)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y), len(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Encoded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:48:32.558698Z",
     "start_time": "2019-02-23T20:48:30.656292Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad labels\n",
    "padded_labels = tf.keras.preprocessing.sequence.pad_sequences(y, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "# convert to categorical\n",
    "categorical_labels = tf.keras.utils.to_categorical(padded_labels, len(tag2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:48:57.345016Z",
     "start_time": "2019-02-23T20:48:55.543911Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad word embedding\n",
    "padded_word = tf.keras.preprocessing.sequence.pad_sequences(x_word, maxlen=max_len, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:50:17.050538Z",
     "start_time": "2019-02-23T20:50:16.444993Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split word embedding and target labels\n",
    "x_word_tr, x_word_te, y_tr, y_te = train_test_split(padded_word, categorical_labels, test_size=0.15, random_state=69)\n",
    "# split character embedding\n",
    "x_char_tr, x_char_te, _, _ = train_test_split(x_char, categorical_labels, test_size=0.15, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert To Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:51:18.474078Z",
     "start_time": "2019-02-23T20:51:12.351334Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert train set\n",
    "x_word_tr = np.array(x_word_tr).reshape((len(x_word_tr), 16))\n",
    "x_char_tr = np.array(x_char_tr).reshape((len(x_char_tr), 16, 8))\n",
    "y_tr = np.array(y_tr).reshape(len(y_tr), 16, 3)\n",
    "\n",
    "# convert dev set\n",
    "x_word_te = np.array(x_word_te).reshape((len(x_word_te), 16))\n",
    "x_char_te = np.array(x_char_te).reshape((len(x_char_te), 16, 8))\n",
    "y_te = np.array(y_te).reshape(len(y_te), 16, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:51:21.832034Z",
     "start_time": "2019-02-23T20:51:21.826606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((343417, 16), (60604, 16))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_word_tr.shape, x_word_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:51:22.107673Z",
     "start_time": "2019-02-23T20:51:22.103041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((343417, 16, 8), (60604, 16, 8))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_char_tr.shape, x_char_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:51:22.398966Z",
     "start_time": "2019-02-23T20:51:22.394538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((343417, 16, 3), (60604, 16, 3))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:54:16.222236Z",
     "start_time": "2019-02-23T20:54:15.159738Z"
    }
   },
   "outputs": [],
   "source": [
    "# input word tensor\n",
    "word_in = tf.keras.Input(shape=(max_len, ))\n",
    "# pass tensor to embedding layer\n",
    "emb_word = tf.keras.layers.Embedding(input_dim=len(word2idx)+2, output_dim=64, input_length=16)(word_in)\n",
    "\n",
    "# input char tensor\n",
    "char_in = tf.keras.Input(shape=(max_len, max_char_len, ))\n",
    "# pass tensor to embedding layer\n",
    "emb_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(input_dim=len(char2idx)+2, output_dim=32, input_length=8))(char_in)\n",
    "\n",
    "# BiLSTM to learn character embeddings\n",
    "char_enc = tf.keras.layers.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64, return_sequences=False, recurrent_dropout=0.4)))(emb_char)\n",
    "\n",
    "# merge word and character embeddings\n",
    "merged = tf.keras.layers.concatenate([emb_word, char_enc])\n",
    "\n",
    "# BiLSTM for ner\n",
    "main_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64, return_sequences=True))(merged)\n",
    "\n",
    "# classify for each word\n",
    "out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=3, activation=\"softmax\"))(main_lstm)\n",
    "\n",
    "# set model together\n",
    "model = tf.keras.Model([word_in, char_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:54:34.160256Z",
     "start_time": "2019-02-23T20:54:34.104818Z"
    }
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:54:35.348340Z",
     "start_time": "2019-02-23T20:54:35.342713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16, 8)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 16, 8, 32)    1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 16, 64)       6106880     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 16, 128)      49664       time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 192)      0           embedding[0][0]                  \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 16, 128)      131584      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 16, 3)        387         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 6,290,307\n",
      "Trainable params: 6,290,307\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T20:56:18.172694Z",
     "start_time": "2019-02-23T20:56:18.168537Z"
    }
   },
   "outputs": [],
   "source": [
    "# model config\n",
    "cbk = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model_char_level.weights.best.hdf5', verbose = True, save_best_only=True, save_weights_only=False),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T00:09:28.230786Z",
     "start_time": "2019-02-23T20:57:02.557635Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nityansuman/anaconda3/envs/ai_env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 343417 samples, validate on 60604 samples\n",
      "Epoch 1/10\n",
      "343417/343417 [==============================] - 1297s 4ms/step - loss: 0.0259 - acc: 0.9922 - val_loss: 0.0150 - val_acc: 0.9954\n",
      "Epoch 2/10\n",
      "343417/343417 [==============================] - 1250s 4ms/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.0115 - val_acc: 0.9964\n",
      "Epoch 3/10\n",
      "343417/343417 [==============================] - 1168s 3ms/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.0096 - val_acc: 0.9971\n",
      "Epoch 4/10\n",
      "343417/343417 [==============================] - 1187s 3ms/step - loss: 0.0078 - acc: 0.9975 - val_loss: 0.0087 - val_acc: 0.9973\n",
      "Epoch 5/10\n",
      "343417/343417 [==============================] - 1184s 3ms/step - loss: 0.0069 - acc: 0.9978 - val_loss: 0.0088 - val_acc: 0.9974\n",
      "Epoch 6/10\n",
      "343417/343417 [==============================] - 1094s 3ms/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0082 - val_acc: 0.9974\n",
      "Epoch 7/10\n",
      "343417/343417 [==============================] - 1088s 3ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0083 - val_acc: 0.9975\n",
      "Epoch 8/10\n",
      "343417/343417 [==============================] - 1088s 3ms/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0085 - val_acc: 0.9975\n",
      "Epoch 9/10\n",
      "343417/343417 [==============================] - 1093s 3ms/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0082 - val_acc: 0.9977\n",
      "Epoch 10/10\n",
      "343417/343417 [==============================] - 1093s 3ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0079 - val_acc: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1adafaab00>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [x_word_tr, x_char_tr], y_tr,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=True,\n",
    "    validation_data=([x_word_te, x_char_te], y_te)\n",
    ")#, callbacks=cbk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai_env]",
   "language": "python",
   "name": "conda-env-ai_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
